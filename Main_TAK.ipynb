{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Package preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9832,"status":"ok","timestamp":1648946079064,"user":{"displayName":"Tram Doan","userId":"12218878138740774355"},"user_tz":-420},"id":"3dkfofXmfSIH","outputId":"cd2524b7-d9d2-46fc-a090-2405cbf7c094"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 11.2 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 46.1 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 56.4 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0\n","Sun Apr  3 00:34:38 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   42C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!pip install transformers\n","!nvidia-smi"]},{"cell_type":"markdown","metadata":{},"source":["## Import necessary packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DHO79vXRzzUW"},"outputs":[],"source":["import torch\n","from torch import nn, Tensor # neural network\n","from torch.nn import functional as F\n","\n","# numerical matrix processing\n","import numpy as np \n","from numpy import ndarray\n","\n","# data/parameter loading\n","import pandas as pd \n","import pickle\n","\n","# visualization\n","from tqdm.notebook import trange, tqdm\n","\n","# transfomers\n","from transformers import AutoTokenizer, AutoModel\n","from transformers.modeling_outputs import BaseModelOutputWithPoolingAndCrossAttentions\n","\n","# code instruction\n","from typing import Union, List, Dict\n","# filter out warnings\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{},"source":["## Some useful functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1NJ2RN7LfAMv"},"outputs":[],"source":["# Utils\n","def save_parameter(save_object, save_file):\n","    with open(save_file, 'wb') as f:\n","        pickle.dump(save_object, f, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","def load_parameter(load_file):\n","    with open(load_file, 'rb') as f:\n","        output = pickle.load(f)\n","    return output\n","\n","def sim_matrix(a, b, eps=1e-8):\n","    \"\"\"\n","    Calculate cosine similarity between two matrices. \n","    Note: added eps for numerical stability\n","    \"\"\"\n","    a_n, b_n = a.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n","    a_norm = a / torch.clamp(a_n, min=eps)\n","    b_norm = b / torch.clamp(b_n, min=eps)\n","    sim_mt = torch.mm(a_norm, b_norm.transpose(0, 1))\n","    return sim_mt\n","\n","def batch2device(batch, device):\n","    \"\"\"\n","    Transfer batch of training to GPU/CPU\n","    Args:\n","        batch: Dict[str, Tensor], represent for transformer input (input_ids, attention_mask)\n","        device: torch.device, GPU or CPU\n","    \"\"\"\n","    for key, value in batch.items():\n","        batch[key] = batch[key].to(device)\n","    return batch\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Data preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hDtULM23fCnm"},"outputs":[],"source":["work_path = \"/content/drive/MyDrive/PaperRecommendation/\"\n","checkpoint_path = work_path + \"checkpoint/\"\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IEtW5FCKfD6M"},"outputs":[],"source":["data_train = pd.read_csv(checkpoint_path + \"data/01_train.csv\", encoding = \"ISO-8859-1\")\n","data_validate = pd.read_csv(checkpoint_path + \"data/01_validate.csv\", encoding = \"ISO-8859-1\")\n","data_test = pd.read_csv(checkpoint_path + \"data/01_test.csv\", encoding = \"ISO-8859-1\")\n","data_aims = pd.read_csv(checkpoint_path + \"data/01_aims.csv\", encoding = \"ISO-8859-1\")\n","\n","data_train.fillna(\"\", inplace=True)\n","data_validate.fillna(\"\", inplace=True)\n","data_test.fillna(\"\", inplace=True)\n","data_aims.fillna(\"\", inplace=True)\n","\n","n_classes = len(data_aims)"]},{"cell_type":"markdown","metadata":{},"source":["## Feature selection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A-gaX5tGfQrJ"},"outputs":[],"source":["X_train = (\n","    data_train['Title']  \n","    + \" \" + data_train['Abstract']\n","    + \" \" + data_train['Keywords']\n","    ).tolist()\n","X_valid = (\n","    data_validate['Title']  \n","    + \" \" + data_validate['Abstract']\n","    + \" \" + data_validate['Keywords']\n","    ).tolist()\n","X_test = (\n","    data_test['Title']  \n","    + \" \" + data_test['Abstract']\n","    + \" \" + data_test['Keywords']\n","    ).tolist()\n","\n","X_aims = data_aims[\"Aims\"].tolist()\n","\n","Y_train = data_train['Label'].tolist()\n","Y_validate = data_validate['Label'].tolist()\n","Y_test = data_test['Label'].tolist()"]},{"cell_type":"markdown","metadata":{},"source":["## Tokenization"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["be01582883334107bce5d8cd434ae6d4","26c73016a362488dac1562498e059da5","e15e344a4e7946bd90d25ef3464638cb","b34216a612ea44a5b9b3588a78ac32c1","2bcbfdd0d3cd450fa56177fa87512312","24c75ba8773c42998d608aec14d7c66b","3109bf320e564b968f1dfac09935f34e","ab7844bb53874a5182f35eb891cf77b1","37e0d86c1f344090937f89080ee2f7cf","27a6ffa1414b4010adce814acc9a64a1","0a1889fe71c14d16b333e3bb6dad6ea6","ba8876a15ca84494809b28eb19793756","f9900b5ff17a4eca84c8938f344e2311","39e18bd5e5694cd4831639d9e13578ba","11f8680efc7f4e60a338afa73fc6f0f5","03c410e6efb2492690f6316308650dd2","4a8c5e286953483e838d40c50cd5b879","43d004c12976409b94681a97b79c36dd","52682f1001524aebaccdcc9db4bd11f0","ac520e50bbbe4d41919cfdf3bfcc1c75","aa86e704b45a44f6842b90701bd60752","18ef45c86af642bfbc5457d2f2e9e72f","b2aefce20d8e49a79edce972e03c8bb5","74cb49bef7ea4c92a45d14f72ef8d3e6","85d94655e29941d2a4ca88c3eddade75","f9404efc8cfe49cbab880d198ee96082","450adf6cb2264150b393a4276db1bbbd","05dd508742cf4653babd7efd5e47d4fb","3f93dfae6fd44751a1a24eeb88e4f034","55bd505ef98e4a65ace61dc3b575820a","c9157b09a17d467f97200d061e0db2cf","6f9a8b139ed14fb6bdf989f7f8ef5e7e","0b701a63c4db4265818951bef08b85d6","b6bb657ddd1a4d62958b923cc881c8f2","a3c3886476cd4f7abb993e0aac518a90","26a1ef50dbe54b44a54657e392db94ce","4bafc01e21ca4d218913fccec05112c8","b11c65cc8cb3402fb92336cae29010d7","c296274bb7e04965a8b9b5fed0afa3e9","47859eee9ca74b0e8f6732d2853f67fe","4a45377147774c079e6b0c84b15fb071","c41dcc9944314d47aad612fcecfcde32","59dbde90bc1241b2829c8b772df5f916","9fb045d01c8b4b3182709a2de83675cd"]},"executionInfo":{"elapsed":2269,"status":"ok","timestamp":1648946168441,"user":{"displayName":"Tram Doan","userId":"12218878138740774355"},"user_tz":-420},"id":"laLk_ODBhiNN","outputId":"52a3b8ec-8ed1-42ba-ae3b-51009597aa24"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"be01582883334107bce5d8cd434ae6d4","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/480 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ba8876a15ca84494809b28eb19793756","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b2aefce20d8e49a79edce972e03c8bb5","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b6bb657ddd1a4d62958b923cc881c8f2","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HLFYv0gRfbVC"},"outputs":[],"source":["# train_encodings = tokenizer(\n","#     X_train,\n","#     truncation=True,\n","#     padding=\"max_length\",\n","#     max_length=300,\n","#     return_tensors=\"pt\"\n","# )\n","# valid_encodings = tokenizer(\n","#     X_valid,\n","#     truncation=True,\n","#     padding=\"max_length\",\n","#     max_length=300,\n","#     return_tensors=\"pt\"\n","# )\n","# test_encodings = tokenizer(\n","#     X_test,\n","#     truncation=True,\n","#     padding=\"max_length\",\n","#     max_length=300,\n","#     return_tensors=\"pt\"\n","# )\n","\n","# save_parameter(train_encodings, checkpoint_path + \"pickle/distilroberta_training_encodings.pickle\")\n","# save_parameter(valid_encodings, checkpoint_path + \"pickle/distilroberta_valid_encodings.pickle\")\n","# save_parameter(test_encodings, checkpoint_path + \"pickle/distilroberta_test_encodings.pickle\")\n","\n","train_encodings = load_parameter(checkpoint_path + \"pickle/distilroberta_training_encodings.pickle\")\n","valid_encodings = load_parameter(checkpoint_path + \"pickle/distilroberta_valid_encodings.pickle\")\n","test_encodings = load_parameter(checkpoint_path + \"pickle/distilroberta_test_encodings.pickle\")"]},{"cell_type":"markdown","metadata":{},"source":["## Data loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kW39BrWJfYcN"},"outputs":[],"source":["class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        x = {\n","            key: torch.tensor(val[idx]) for key, val in self.encodings.items()\n","        }\n","        y = torch.tensor(self.labels[idx])\n","        return x, y\n","    def __len__(self):\n","        return len(self.labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a37MYLlNfdqy"},"outputs":[],"source":["# Dataset\n","train_dataset = Dataset(train_encodings, Y_train)\n","valid_dataset = Dataset(valid_encodings, Y_validate)\n","test_dataset = Dataset(test_encodings, Y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z0xxnYtJffb3"},"outputs":[],"source":["# Data loaders\n","train_loader = torch.utils.data.DataLoader(train_dataset,\n","                                         batch_size=16,\n","                                         shuffle=True)\n","valid_loader = torch.utils.data.DataLoader(valid_dataset,\n","                                         batch_size=8,\n","                                         shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_dataset,\n","                                         batch_size=8,\n","                                         shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Model definition"]},{"cell_type":"markdown","metadata":{},"source":["## Pooler layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wTOopKIvfhAx"},"outputs":[],"source":["class Pooler(nn.Module):\n","    \"\"\"\n","    Parameter-free poolers to get the sentence embedding\n","    'cls': [CLS] representation with BERT/RoBERTa's MLP pooler.\n","    'cls_before_pooler': [CLS] representation without the original MLP pooler.\n","    'avg': average of the last layers' hidden states at each token.\n","    'avg_top2': average of the last two layers.\n","    'avg_first_last': average of the first and the last layers.\n","    \"\"\"\n","    def __init__(self, pooler_type):\n","        super().__init__()\n","        self.pooler_type = pooler_type\n","        assert self.pooler_type in [\"cls\", \"cls_before_pooler\", \"avg\", \"avg_top2\", \"avg_first_last\"], \"unrecognized pooling type %s\" % self.pooler_type\n","\n","    def forward(self, attention_mask, outputs):\n","        last_hidden = outputs.last_hidden_state\n","        hidden_states = outputs.hidden_states\n","\n","        if self.pooler_type in ['cls_before_pooler', 'cls']:\n","            return last_hidden[:, 0]\n","        elif self.pooler_type == \"avg\":\n","            return ((last_hidden * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(-1).unsqueeze(-1))\n","        elif self.pooler_type == \"avg_first_last\":\n","            first_hidden = hidden_states[0]\n","            last_hidden = hidden_states[-1]\n","            pooled_result = ((first_hidden + last_hidden) / 2.0 * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(-1).unsqueeze(-1)\n","            return pooled_result\n","        elif self.pooler_type == \"avg_top2\":\n","            second_last_hidden = hidden_states[-2]\n","            last_hidden = hidden_states[-1]\n","            pooled_result = ((last_hidden + second_last_hidden) / 2.0 * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(-1).unsqueeze(-1)\n","            return pooled_result\n","        else:\n","            raise NotImplementedError\n"]},{"cell_type":"markdown","metadata":{},"source":["## Sentence Embedder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bS77oJSrfiLq"},"outputs":[],"source":["class ModelForSE(nn.Module):\n","    def __init__(self, model_name_or_path, pooler_type):\n","        super(ModelForSE, self).__init__()\n","        '''\n","        Model for sentence embedding\n","        '''\n","        self.bert = AutoModel.from_pretrained(model_name_or_path)\n","        self.pooler_type = pooler_type\n","        self.pooler = Pooler(self.pooler_type)\n","        \n","    def forward(self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","        mlm_input_ids=None,\n","        mlm_labels=None,\n","    ):\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=True if self.pooler_type in ['avg_top2', 'avg_first_last'] else False,\n","            return_dict=return_dict,\n","        )\n","        if self.pooler_type in [\"cls\", \"cls_before_pooler\", \"avg\", \"avg_top2\", \"avg_first_last\"]:\n","            pooler_output = self.pooler(attention_mask, outputs)\n","        \n","        return BaseModelOutputWithPoolingAndCrossAttentions(\n","            pooler_output=pooler_output,\n","            last_hidden_state=outputs.last_hidden_state,\n","            hidden_states=outputs.hidden_states,\n","        )\n","    def encode(self, sentences: Union[str, List[str]],\n","               batch_size: int = 8,\n","               show_progress_bar: bool = None,\n","               convert_to_numpy: bool = True,\n","               convert_to_tensor: bool = False,\n","               device: str = None) -> Union[List[Tensor], ndarray, Tensor]:\n","        self.eval()\n","\n","        if convert_to_tensor:\n","            convert_to_numpy = False\n","\n","        input_was_string = False\n","\n","        if isinstance(sentences, str) or not hasattr(sentences, '__len__'): #Cast an individual sentence to a list with length 1\n","            sentences = [sentences]\n","            input_was_string = True\n","\n","        if device is None:\n","            device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","        self.to(device)\n","\n","        all_embeddings = []\n","        for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\", disable=not show_progress_bar):\n","            sentence_batch = sentences[start_index: start_index+batch_size]\n","            features = tokenizer(sentence_batch,\n","                       padding='max_length', \n","                       truncation=True, \n","                       max_length=300,\n","                       return_tensors='pt').to(device)\n","            \n","            with torch.no_grad():\n","                out_features = self.forward(**features)\n","                embeddings = []\n","                # gather the embedding vectors\n","                for row in out_features.pooler_output:\n","                    embeddings.append(row.cpu())\n","                all_embeddings.extend(embeddings)\n","        if convert_to_tensor:\n","            all_embeddings = torch.vstack(all_embeddings)\n","        elif convert_to_numpy:\n","            all_embeddings = np.asarray([emb.numpy() for emb in all_embeddings])\n","        \n","        if input_was_string:\n","            all_embeddings = all_embeddings[0]\n","        return all_embeddings"]},{"cell_type":"markdown","metadata":{},"source":["## Load fine-tuned LM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140,"referenced_widgets":["e46f6dac7bc04202923e0ec1c8a6c30f","a045ab1826fb48218d0f81206acb319d","be9819deaf134c7793e0bf1d7bb950d8","62ef600785fd4a039184589d28d9ee7e","9c8f142f6dd248d180ce0b70f91354fc","31f9bc645534443fbfc4739a95f30cf3","30247cbc87e84f74a4ef81b464ef4f40","b70ba7590cc6463cbca8834f2e223922","3863ef849e0f40b7a57a56c85d3def31","121157ae1a334c5b8052acb693b50b74","609ce402f4cf4c42a6b79e97413bed66"]},"executionInfo":{"elapsed":11123,"status":"ok","timestamp":1648946201384,"user":{"displayName":"Tram Doan","userId":"12218878138740774355"},"user_tz":-420},"id":"CrB0d4rgfl26","outputId":"4392870d-2ea3-42c7-d7f7-b7777b1adfd1"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e46f6dac7bc04202923e0ec1c8a6c30f","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/316M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# Fine-tuned LM checkpoint (by contrastive learning)\n","checkpoint_cl = torch.load(checkpoint_path + \"Epoch:09 New-SupCL-DistilRoBERTa.pth\")\n","\n","\n","# Baseline model of sentence embeddings\n","model_args = {\n","    \"model_name_or_path\": \"distilroberta-base\",\n","    \"pooler_type\": \"cls_before_pooler\"\n","}\n","base_model = ModelForSE(**model_args)\n","base_model.load_state_dict(checkpoint_cl[\"model_state_dict\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OSydTiZ4fn4R"},"outputs":[],"source":["class NoAim_Classifier(nn.Module):\n","    def __init__(self, base_model, num_classes):\n","        super(NoAim_Classifier, self).__init__()\n","        self.base_model = base_model\n","        self.linear1_1 = nn.Linear(768, 512)\n","        self.act1_1 = nn.ReLU()\n","        self.drop1_1 = nn.Dropout(0.1)\n","\n","        self.linear1_2 = nn.Linear(512, num_classes)\n","        self.logsoftmax = nn.LogSoftmax(dim=1) \n","\n","    def forward(self, inputs_tak):\n","        '''\n","        Args:\n","            inputs_tak: (dict) batch of TAK samples, shape as [bs, n_samples, encoding_dim]\n","            inputs_aims: (tensor) batch of aims embeddings taken by cls tokens, shape as [bs, n_samples, hidden_size]\n","        '''\n","        output_tak = self.base_model(**inputs_tak)\n","        x = output_tak.last_hidden_state[:,0,:] # cls tokens\n","        x = self.linear1_1(x)\n","        x = self.act1_1(x)\n","        x = self.drop1_1(x)\n","        \n","        x = self.linear1_2(x)\n","        return self.logsoftmax(x)"]},{"cell_type":"markdown","metadata":{"id":"nYN3sWYdtm3J"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1648946201385,"user":{"displayName":"Tram Doan","userId":"12218878138740774355"},"user_tz":-420},"id":"uNHqajOAfz-J","outputId":"5c9acab3-030f-4634-bb00-083a5492dea9"},"outputs":[{"data":{"text/plain":["NoAim_Classifier(\n","  (base_model): ModelForSE(\n","    (bert): RobertaModel(\n","      (embeddings): RobertaEmbeddings(\n","        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","        (position_embeddings): Embedding(514, 768, padding_idx=1)\n","        (token_type_embeddings): Embedding(1, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): RobertaEncoder(\n","        (layer): ModuleList(\n","          (0): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): RobertaPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (pooler): Pooler()\n","  )\n","  (linear1_1): Linear(in_features=768, out_features=512, bias=True)\n","  (act1_1): ReLU()\n","  (drop1_1): Dropout(p=0.1, inplace=False)\n","  (linear1_2): Linear(in_features=512, out_features=351, bias=True)\n","  (logsoftmax): LogSoftmax(dim=1)\n",")"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# load checkpoint and continue training\n","model = NoAim_Classifier(base_model, n_classes)\n","model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["## Optimizer and Loss function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5TZqFHAcf4mK"},"outputs":[],"source":["# Optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n","lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.96)\n","\n","# Loss function\n","loss_fn = nn.NLLLoss().to(device)"]},{"cell_type":"markdown","metadata":{},"source":["## Training settings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zbkFjLPHf5uv"},"outputs":[],"source":["max_epochs = 7\n","topks = [1, 3, 5, 10]\n","history = {\n","    \"train_loss\": [],\n","    \"val_loss\": [],\n","    \"train_acc@k\": [],\n","    \"val_acc@k\": [],\n","}\n","min_valid_loss = np.inf"]},{"cell_type":"markdown","metadata":{},"source":["## Training loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9PWI1t-nf7Ev"},"outputs":[],"source":["for epoch in range(max_epochs):\n","    train_loss = 0.0\n","    train_loop = tqdm(train_loader, leave=True)\n","    batch_train_accuracy = {k: 0 for k in topks}\n","    batch_valid_accuracy = {k: 0 for k in topks}\n","    num_correct_at_k = {\n","        \"train\": {k: 0 for k in topks},\n","        \"val\": {k: 0 for k in topks}\n","    }\n","    # Training\n","    model.train()\n","\n","    for features, labels in train_loop:\n","                \n","        # Transfer Data to GPU if available\n","        if torch.cuda.is_available():\n","            features, labels = batch2device(features, device), labels.to(device)\n","        # forward pass\n","        logits = model(features)\n","        # Clear the gradients\n","        optimizer.zero_grad()\n","        # Find the Loss\n","        loss = loss_fn(logits, labels)\n","        # Calculate gradients\n","        loss.backward()\n","        # Update Weights\n","        optimizer.step()\n","        # Calculate accuracy\n","        probs_des = torch.argsort(torch.exp(logits), axis=1, descending=True)\n","        for k in topks:\n","            batch_num_correct = 0\n","            nPoints = len(labels)\n","            for i in range(nPoints):\n","                if labels[i] in probs_des[i, 0:k]:\n","                    batch_num_correct += 1\n","                    num_correct_at_k[\"train\"][k] += 1 # globally counting number of correct at each k's for whole valid set\n","            batch_train_accuracy[k] = batch_num_correct / nPoints\n","        # Calculate Loss\n","        train_loss += loss.item()\n","        train_loop.set_description('Epoch: {0} - lr: {1}, Training'.format(epoch, optimizer.param_groups[0]['lr']))\n","        train_loop.set_postfix(train_loss=loss.item(), \n","                               top01=batch_train_accuracy[1], \n","                               top03=batch_train_accuracy[3], \n","                               top05=batch_train_accuracy[5],\n","                               top10=batch_train_accuracy[10])\n","    train_loss = train_loss/len(train_loader)\n","    history[\"train_loss\"].append(train_loss)\n","    history[\"train_acc@k\"].append(\n","        {k: val/len(X_train) for k, val in num_correct_at_k[\"train\"].items()}\n","    )\n","\n","    # Validation\n","    valid_loss = 0.0\n","    valid_loop = tqdm(valid_loader, leave=True)\n","    with torch.no_grad():\n","        model.eval()\n","        # Transfer Data to GPU if available\n","        for features, labels in valid_loop:\n","\n","            if torch.cuda.is_available():\n","                features, labels = batch2device(features, device), labels.to(device)\n","            # Forward pass\n","            logits = model(features)\n","            \n","            # Find the Loss\n","            loss = loss_fn(logits, labels)\n","            # Calculate accuracy\n","            probs_des = torch.argsort(torch.exp(logits), axis=1, descending=True)\n","            for k in topks:\n","                num_correct = 0\n","                nPoints = len(labels)\n","                for i in range(nPoints):\n","                    if labels[i] in probs_des[i, 0:k]:\n","                        num_correct += 1\n","                        num_correct_at_k[\"val\"][k] += 1 # globally counting number of correct at each k's for whole valid set\n","                batch_valid_accuracy[k] = num_correct / nPoints\n","            # Calculate Loss\n","            valid_loss += loss.item()\n","            valid_loop.set_description('Epoch: {0} - lr: {1}, Validating'.format(epoch, optimizer.param_groups[0]['lr']))\n","            valid_loop.set_postfix(val_loss=loss.item(), \n","                                val_top01=batch_valid_accuracy[1], \n","                                val_top03=batch_valid_accuracy[3], \n","                                val_top05=batch_valid_accuracy[5],\n","                                val_top10=batch_valid_accuracy[10])\n","        valid_loss = valid_loss/len(valid_loader)\n","        history[\"val_loss\"].append(valid_loss)\n","        history[\"val_acc@k\"].append(\n","            {k: val/len(X_valid) for k, val in num_correct_at_k[\"val\"].items()}\n","        )\n","        print(f'>> Epoch {epoch} \\t\\t Training Loss: {train_loss} \\t\\t Validation Loss: {valid_loss}')\n","        # lr_scheduler.step()\n","\n","        if min_valid_loss > valid_loss:\n","            print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n","            min_valid_loss = valid_loss\n","            \n","            # Saving State Dict\n","            torch.save(\n","                {\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'history': history,\n","                    'epoch': epoch\n","                }, checkpoint_path + \"weight/Epoch:{:0>2} DistilRoberta_TAK.pth\".format(epoch)\n","            )"]},{"cell_type":"markdown","metadata":{"id":"l7wNgQATrBr6"},"source":["# Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SOP0nAFxtq5V"},"outputs":[],"source":["# load checkpoint and testing\n","checkpoint = torch.load(checkpoint_path + \"weight/Epoch:03 DisRoberta(TAK).pth\")\n","\n","model = NoAim_Classifier(base_model, n_classes)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","model.to(device)\n","\n","history = checkpoint['history']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["93b7133600304991a0251d314de3fb05","f04eac96ffbf4df781d56dcc5156b494","7fcc53b44dcd489bbab38fe62a55a1a5","37a16a230e9f408e93ed30ea9fff8651","f400ca8d119742c2b0d10b87b3ec444b","5c73008ab05c42719e798021cc442ce8","ba81e1fdeb444b2db33ce19b93649bd6","3ded95c3595f4e6abf6fdcf29f4bf94d","01364a7295bf4d7d9abee4d8bdf13963","f934c3b1bf3e4c6693b843bee40b7751","8a27bf1619e349a79079802f2f0f78e4"]},"executionInfo":{"elapsed":574918,"status":"ok","timestamp":1648946790506,"user":{"displayName":"Tram Doan","userId":"12218878138740774355"},"user_tz":-420},"id":"Vr3eDjKAf90e","outputId":"95d49cb0-5311-4f74-acc5-ce8cc20cf44a"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"93b7133600304991a0251d314de3fb05","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/10381 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Loss function\n","loss_fn = nn.NLLLoss().to(device)\n","\n","# Test \n","topks = [1, 3, 5, 10]\n","num_correct_at_k = {}\n","test_loop = tqdm(test_loader, leave=True)\n","num_correct_at_k[\"test\"] = {k: 0 for k in topks}\n","batch_test_accuracy = {k: [] for k in topks}\n","history[\"test_acc@k\"] = []\n","history[\"test_loss\"] = []\n","test_loss = 0.0\n","\n","with torch.no_grad():\n","    model.eval() \n","    for features, labels in test_loop:\n","        # Transfer Data to GPU if available\n","        if torch.cuda.is_available():\n","            features, labels = batch2device(features, device), labels.to(device)\n","        logits = model(features)\n","        # Find the Loss\n","        loss = loss_fn(logits, labels)\n","        # Calculate accuracy\n","        probs_des = torch.argsort(torch.exp(logits), axis=1, descending=True)\n","        for k in topks:\n","            num_correct = 0\n","            nPoints = len(labels)\n","            for i in range(nPoints):\n","                if labels[i] in probs_des[i, 0:k]:\n","                    num_correct += 1\n","                    num_correct_at_k[\"test\"][k] += 1 # globally counting number of correct at each k's for whole valid set\n","            batch_test_accuracy[k] = num_correct / nPoints\n","        # Calculate Loss\n","        test_loss += loss.item()\n","        test_loop.set_description('Testing...')\n","        test_loop.set_postfix(test_loss=loss.item(), \n","                            test_top01=batch_test_accuracy[1], \n","                            test_top03=batch_test_accuracy[3], \n","                            test_top05=batch_test_accuracy[5],\n","                            test_top10=batch_test_accuracy[10])\n","    test_loss = test_loss/len(test_loader)\n","    history[\"test_loss\"].append(test_loss)\n","    history[\"test_acc@k\"].append(\n","        {k: val/len(X_test) for k, val in num_correct_at_k[\"test\"].items()}\n","    )"]},{"cell_type":"markdown","metadata":{"id":"lJ60IrJ4q9cG"},"source":["# Final results"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1648946790507,"user":{"displayName":"Tram Doan","userId":"12218878138740774355"},"user_tz":-420},"id":"DPF5969EqV6X","outputId":"45a0f968-9d3e-44af-c5c8-b8bad87731f9"},"outputs":[{"name":"stdout","output_type":"stream","text":[">> Final results (Best model): \n","\tTraining loss: 1.2283474295100631\n","\tValidating loss: 1.4882379100122218\n","\tTesting loss: 1.4920472683294104\n","\n","\n","\tTrain accuracy@1: 0.5709195252030558\n","\tTrain accuracy@3: 0.8553250401418625\n","\tTrain accuracy@5: 0.9213387101640201\n","\tTrain accuracy@10: 0.9711950710150612\n","\n","\n","\tValidate accuracy@1: 0.5190816665158234\n","\tValidate accuracy@3: 0.8107521042628292\n","\tValidate accuracy@5: 0.8883156846773463\n","\tValidate accuracy@10: 0.9507647750927686\n","\n","\n","\tTest accuracy@1: 0.5173032463153838\n","\tTest accuracy@3: 0.8097124554474521\n","\tTest accuracy@5: 0.8862344668143725\n","\tTest accuracy@10: 0.9495592910124265\n"]}],"source":["print(\">> Final results (Best model): \")\n","print(\"\\tTraining loss: {}\".format(history[\"train_loss\"][-1]))\n","print(\"\\tValidating loss: {}\".format(history[\"val_loss\"][-1]))\n","print(\"\\tTesting loss: {}\".format(history[\"test_loss\"][-1]))\n","print(\"\\n\")\n","for k in topks:\n","    print(\"\\tTrain accuracy@{}: {}\".format(k, history[\"train_acc@k\"][-1][k]))\n","print(\"\\n\")\n","for k in topks:\n","    print(\"\\tValidate accuracy@{}: {}\".format(k, history[\"val_acc@k\"][-1][k]))\n","print(\"\\n\")\n","for k in topks:\n","    print(\"\\tTest accuracy@{}: {}\".format(k, history[\"test_acc@k\"][-1][k]))"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyP9xb7eouu0lQFteFVg/Gch","collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1kGORnWq_vArEjvKGAMUwZRzgdeEJBux8","name":"DistilRoberta_TAK.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"01364a7295bf4d7d9abee4d8bdf13963":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"03c410e6efb2492690f6316308650dd2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05dd508742cf4653babd7efd5e47d4fb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a1889fe71c14d16b333e3bb6dad6ea6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0b701a63c4db4265818951bef08b85d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"11f8680efc7f4e60a338afa73fc6f0f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa86e704b45a44f6842b90701bd60752","placeholder":"​","style":"IPY_MODEL_18ef45c86af642bfbc5457d2f2e9e72f","value":" 878k/878k [00:00&lt;00:00, 6.34MB/s]"}},"121157ae1a334c5b8052acb693b50b74":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18ef45c86af642bfbc5457d2f2e9e72f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"24c75ba8773c42998d608aec14d7c66b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26a1ef50dbe54b44a54657e392db94ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a45377147774c079e6b0c84b15fb071","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c41dcc9944314d47aad612fcecfcde32","value":1355863}},"26c73016a362488dac1562498e059da5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_24c75ba8773c42998d608aec14d7c66b","placeholder":"​","style":"IPY_MODEL_3109bf320e564b968f1dfac09935f34e","value":"Downloading: 100%"}},"27a6ffa1414b4010adce814acc9a64a1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2bcbfdd0d3cd450fa56177fa87512312":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30247cbc87e84f74a4ef81b464ef4f40":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3109bf320e564b968f1dfac09935f34e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"31f9bc645534443fbfc4739a95f30cf3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37a16a230e9f408e93ed30ea9fff8651":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f934c3b1bf3e4c6693b843bee40b7751","placeholder":"​","style":"IPY_MODEL_8a27bf1619e349a79079802f2f0f78e4","value":" 10381/10381 [09:34&lt;00:00, 18.15it/s, test_loss=1.61, test_top01=0.75, test_top03=0.875, test_top05=0.875, test_top10=0.875]"}},"37e0d86c1f344090937f89080ee2f7cf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3863ef849e0f40b7a57a56c85d3def31":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"39e18bd5e5694cd4831639d9e13578ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_52682f1001524aebaccdcc9db4bd11f0","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ac520e50bbbe4d41919cfdf3bfcc1c75","value":898823}},"3ded95c3595f4e6abf6fdcf29f4bf94d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f93dfae6fd44751a1a24eeb88e4f034":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"43d004c12976409b94681a97b79c36dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"450adf6cb2264150b393a4276db1bbbd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47859eee9ca74b0e8f6732d2853f67fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a45377147774c079e6b0c84b15fb071":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a8c5e286953483e838d40c50cd5b879":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bafc01e21ca4d218913fccec05112c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_59dbde90bc1241b2829c8b772df5f916","placeholder":"​","style":"IPY_MODEL_9fb045d01c8b4b3182709a2de83675cd","value":" 1.29M/1.29M [00:00&lt;00:00, 3.67MB/s]"}},"52682f1001524aebaccdcc9db4bd11f0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55bd505ef98e4a65ace61dc3b575820a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59dbde90bc1241b2829c8b772df5f916":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c73008ab05c42719e798021cc442ce8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"609ce402f4cf4c42a6b79e97413bed66":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"62ef600785fd4a039184589d28d9ee7e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_121157ae1a334c5b8052acb693b50b74","placeholder":"​","style":"IPY_MODEL_609ce402f4cf4c42a6b79e97413bed66","value":" 316M/316M [00:09&lt;00:00, 38.9MB/s]"}},"6f9a8b139ed14fb6bdf989f7f8ef5e7e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74cb49bef7ea4c92a45d14f72ef8d3e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_05dd508742cf4653babd7efd5e47d4fb","placeholder":"​","style":"IPY_MODEL_3f93dfae6fd44751a1a24eeb88e4f034","value":"Downloading: 100%"}},"7fcc53b44dcd489bbab38fe62a55a1a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ded95c3595f4e6abf6fdcf29f4bf94d","max":10381,"min":0,"orientation":"horizontal","style":"IPY_MODEL_01364a7295bf4d7d9abee4d8bdf13963","value":10381}},"85d94655e29941d2a4ca88c3eddade75":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_55bd505ef98e4a65ace61dc3b575820a","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c9157b09a17d467f97200d061e0db2cf","value":456318}},"8a27bf1619e349a79079802f2f0f78e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93b7133600304991a0251d314de3fb05":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f04eac96ffbf4df781d56dcc5156b494","IPY_MODEL_7fcc53b44dcd489bbab38fe62a55a1a5","IPY_MODEL_37a16a230e9f408e93ed30ea9fff8651"],"layout":"IPY_MODEL_f400ca8d119742c2b0d10b87b3ec444b"}},"9c8f142f6dd248d180ce0b70f91354fc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9fb045d01c8b4b3182709a2de83675cd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a045ab1826fb48218d0f81206acb319d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_31f9bc645534443fbfc4739a95f30cf3","placeholder":"​","style":"IPY_MODEL_30247cbc87e84f74a4ef81b464ef4f40","value":"Downloading: 100%"}},"a3c3886476cd4f7abb993e0aac518a90":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c296274bb7e04965a8b9b5fed0afa3e9","placeholder":"​","style":"IPY_MODEL_47859eee9ca74b0e8f6732d2853f67fe","value":"Downloading: 100%"}},"aa86e704b45a44f6842b90701bd60752":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab7844bb53874a5182f35eb891cf77b1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac520e50bbbe4d41919cfdf3bfcc1c75":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b11c65cc8cb3402fb92336cae29010d7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2aefce20d8e49a79edce972e03c8bb5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_74cb49bef7ea4c92a45d14f72ef8d3e6","IPY_MODEL_85d94655e29941d2a4ca88c3eddade75","IPY_MODEL_f9404efc8cfe49cbab880d198ee96082"],"layout":"IPY_MODEL_450adf6cb2264150b393a4276db1bbbd"}},"b34216a612ea44a5b9b3588a78ac32c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_27a6ffa1414b4010adce814acc9a64a1","placeholder":"​","style":"IPY_MODEL_0a1889fe71c14d16b333e3bb6dad6ea6","value":" 480/480 [00:00&lt;00:00, 15.0kB/s]"}},"b6bb657ddd1a4d62958b923cc881c8f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a3c3886476cd4f7abb993e0aac518a90","IPY_MODEL_26a1ef50dbe54b44a54657e392db94ce","IPY_MODEL_4bafc01e21ca4d218913fccec05112c8"],"layout":"IPY_MODEL_b11c65cc8cb3402fb92336cae29010d7"}},"b70ba7590cc6463cbca8834f2e223922":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba81e1fdeb444b2db33ce19b93649bd6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ba8876a15ca84494809b28eb19793756":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f9900b5ff17a4eca84c8938f344e2311","IPY_MODEL_39e18bd5e5694cd4831639d9e13578ba","IPY_MODEL_11f8680efc7f4e60a338afa73fc6f0f5"],"layout":"IPY_MODEL_03c410e6efb2492690f6316308650dd2"}},"be01582883334107bce5d8cd434ae6d4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_26c73016a362488dac1562498e059da5","IPY_MODEL_e15e344a4e7946bd90d25ef3464638cb","IPY_MODEL_b34216a612ea44a5b9b3588a78ac32c1"],"layout":"IPY_MODEL_2bcbfdd0d3cd450fa56177fa87512312"}},"be9819deaf134c7793e0bf1d7bb950d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b70ba7590cc6463cbca8834f2e223922","max":331070498,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3863ef849e0f40b7a57a56c85d3def31","value":331070498}},"c296274bb7e04965a8b9b5fed0afa3e9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c41dcc9944314d47aad612fcecfcde32":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c9157b09a17d467f97200d061e0db2cf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e15e344a4e7946bd90d25ef3464638cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab7844bb53874a5182f35eb891cf77b1","max":480,"min":0,"orientation":"horizontal","style":"IPY_MODEL_37e0d86c1f344090937f89080ee2f7cf","value":480}},"e46f6dac7bc04202923e0ec1c8a6c30f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a045ab1826fb48218d0f81206acb319d","IPY_MODEL_be9819deaf134c7793e0bf1d7bb950d8","IPY_MODEL_62ef600785fd4a039184589d28d9ee7e"],"layout":"IPY_MODEL_9c8f142f6dd248d180ce0b70f91354fc"}},"f04eac96ffbf4df781d56dcc5156b494":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c73008ab05c42719e798021cc442ce8","placeholder":"​","style":"IPY_MODEL_ba81e1fdeb444b2db33ce19b93649bd6","value":"Testing...: 100%"}},"f400ca8d119742c2b0d10b87b3ec444b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f934c3b1bf3e4c6693b843bee40b7751":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9404efc8cfe49cbab880d198ee96082":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f9a8b139ed14fb6bdf989f7f8ef5e7e","placeholder":"​","style":"IPY_MODEL_0b701a63c4db4265818951bef08b85d6","value":" 446k/446k [00:00&lt;00:00, 4.70MB/s]"}},"f9900b5ff17a4eca84c8938f344e2311":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a8c5e286953483e838d40c50cd5b879","placeholder":"​","style":"IPY_MODEL_43d004c12976409b94681a97b79c36dd","value":"Downloading: 100%"}}}}},"nbformat":4,"nbformat_minor":0}
